## Fine-Tuning GPT-2 fÃ¼r kreatives Schreiben

Dieses Projekt befasst sich mit dem Finetuning eines GPT-2-Modells zur Generierung kreativer Texte. Es umfasst das Training des Modells mit einer benutzerdefinierten Datensammlung, die Generierung neuer Texte sowie Unit-Tests zur Sicherstellung der FunktionalitÃ¤t.


## Verzeichnisstruktur
```
ğŸ“ project_root
â”œâ”€â”€ ğŸ“ fine_tune_gpt2.py          # Skript zum Trainieren von GPT-2
â”œâ”€â”€ ğŸ“ generate_text.py           # Skript zur Textgenerierung
â”œâ”€â”€ ğŸ§ª test_fine_tuned_gpt2.py    # Unittests fÃ¼r das trainierte Modell
â”œâ”€â”€ ğŸ“„ creative_texts.txt         # Trainingsdaten fÃ¼r das Modell
â”œâ”€â”€ ğŸ“– README.md                  # Projektdokumentation
```

## Nutzung
### 1ï¸âƒ£ **Finetuning des Modells**
```bash
python fine_tune_gpt2.py
```
Das Modell wird mit den in `creative_texts.txt` gespeicherten Beispielen trainiert und im Verzeichnis `./fine_tuned_gpt2` gespeichert.

### 2ï¸âƒ£ **Generierung von Texten**
```bash
python generate_text.py
```
Das Skript lÃ¤dt das trainierte Modell und generiert Texte basierend auf einer vorgegebenen Eingabe.

### 3ï¸âƒ£ **Tests ausfÃ¼hren**
```bash
python -m unittest test_fine_tuned_gpt2.py
```
Die Tests Ã¼berprÃ¼fen, ob das Modell korrekt trainiert wurde und Texte sinnvoll generiert.

## Anpassungen
- Die Trainingsdaten in `creative_texts.txt` kÃ¶nnen erweitert oder angepasst werden.
- Die Hyperparameter fÃ¼r das Training und die Textgenerierung (z.B. `temperature`, `top_k`, `top_p`) kÃ¶nnen in den jeweiligen Skripten modifiziert werden.



